{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from scipy import sparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pd = pd.read_csv(\"./data/train.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 27500, 1: 2500})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_pd.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2500/3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 91490.53it/s]\n"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "vocab_list = []\n",
    "for line in tqdm(train_pd.value):\n",
    "    word_list.append([w for w in line])\n",
    "    vocab_list.extend([w for w in line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['%', '&', '*', '+', '-', '.', '0', '1', '2', '3', '4', '5', '6',\n",
       "       '7', '8', '9', '=', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I',\n",
       "       'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V',\n",
       "       'W', 'X', 'Y', 'Z', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h',\n",
       "       'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u',\n",
       "       'v', 'w', 'x', 'y', 'z'],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter(vocab_list)\n",
    "vocab = np.array(list(c.keys()))\n",
    "vocab.sort()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(vocab)\n",
    "label_encoder.classes_\n",
    "label_encoder.transform(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_labelencoder = [list(label_encoder.transform(w)) for w in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['k', 'e', 'y', 'o', 'n', 'e', '=', 'R', '8', '9', 'y', '0', 'w',\n",
       "       'd', 'I', 'l', 'T', 'H', 'A', '2', 'z', 'g', 'y', 'd', '1', 'C',\n",
       "       'C', '&', 'k', 'e', 'y', 't', 'w', 'o', '=', 'j', 'Q', 'u', 'e',\n",
       "       'r', 'y', '2', '1', '4', '0', '5', '6', '2', '4', '6', '5', '1',\n",
       "       '8', '8', '0', '3', '5', '2', '1', '6', '7', '_', '1', '4', '7',\n",
       "       '8', '1', '4', '5', '2', '3', '3', '2', '2', '2'],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_labelencoder[0]\n",
    "label_encoder.inverse_transform(word_labelencoder[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30000,), (30000,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch = np.array(word_labelencoder.copy())\n",
    "y_batch = np.array(train_pd.label.copy())\n",
    "# len(x), len(y)\n",
    "x_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pos = x_batch[y_batch==1]\n",
    "x_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_neg = x_batch[y_batch==0]\n",
    "x_neg = np.random.choice(x_neg, size=2500)\n",
    "x_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2500,), (2500,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pos.shape, x_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_batch = np.append(x_pos, x_neg)\n",
    "# x = np.array(list(x_pos).extend(x_neg))\n",
    "y_batch = np.concatenate([[1]*len(x_pos), [0]*len(x_neg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ list([54, 48, 68, 58, 57, 48, 16, 53, 62, 58, 57, 59, 14, 6, 13, 1, 54, 48, 68, 63, 66, 58, 16, 49, 58, 58, 0, 8, 11, 8, 13, 0, 8, 11, 8, 15, 0, 8, 11, 8, 15, 0, 8, 11, 8, 15, 0, 8, 11, 8, 6, 39, 17, 25, 36, 22, 31, 34, 0, 8, 11, 8, 6, 20, 21, 28, 17, 41, 0, 8, 11, 8, 6, 0, 8, 11, 8, 13, 6, 0, 8, 11, 9, 17, 6, 0, 8, 11, 9, 17, 11, 0, 8, 11, 8, 13, 4, 4, 1, 54, 48, 68, 63, 51, 61, 48, 48, 16, 13, 15, 12, 10, 13, 6, 10, 8, 9, 10]),\n",
       "       list([54, 48, 68, 58, 57, 48, 16, 13, 15, 13, 11, 14, 13, 12, 9, 9, 14, 1, 54, 48, 68, 63, 66, 58, 16, 6, 3, 37, 30, 25, 31, 30, 3, 35, 21, 28, 21, 19, 36, 3, 17, 21, 35, 43, 20, 21, 19, 34, 41, 32, 36, 0, 8, 14, 17, 21, 35, 43, 21, 30, 19, 34, 41, 32, 36, 0, 8, 14, 19, 31, 30, 19, 17, 36, 0, 8, 14, 6, 67, 13, 14, 13, 9, 13, 7, 12, 19, 12, 15, 12, 21, 12, 17, 12, 8, 12, 11, 12, 13, 12, 15, 12, 21, 0, 8, 19, 0, 8, 14, 35, 21, 28, 21, 19, 36, 3, 19, 31, 30, 19, 17, 36, 0, 8, 14, 36, 17, 18, 28, 21, 43, 30, 17, 29, 21, 0, 8, 19, 6, 67, 13, 14, 13, 9, 13, 7, 12, 19, 12, 15, 12, 21, 12, 17, 12, 10, 12, 11, 12, 19, 0, 8, 19, 36, 17, 18, 28, 21, 43, 35, 19, 24, 21, 29, 17, 0, 8, 15, 3, 22, 34, 31, 29, 3, 25, 30, 22, 31, 34, 29, 17, 36, 25, 31, 30, 43, 35, 19, 24, 21, 29, 17, 5, 36, 17, 18, 28, 21, 35, 3, 28, 25, 29, 25, 36, 3, 13, 0, 8, 19, 7, 0, 8, 15, 0, 8, 19, 6, 67, 13, 14, 13, 9, 13, 7, 12, 19, 12, 15, 12, 21, 12, 17, 12, 11, 12, 21, 12, 10, 0, 8, 15, 0, 8, 19, 6, 67, 13, 7, 0, 8, 15, 0, 8, 19, 6, 67, 13, 7, 0, 8, 15, 4, 4, 1, 54, 48, 68, 63, 51, 61, 48, 48, 16, 7, 15, 13, 12, 14, 9, 15, 12, 10, 6]),\n",
       "       list([54, 48, 68, 58, 57, 48, 16, 4, 12, 15, 10, 11, 0, 8, 11, 8, 6, 37, 30, 25, 31, 30, 0, 8, 11, 8, 6, 17, 28, 28, 0, 8, 11, 8, 6, 35, 21, 28, 21, 19, 36, 0, 8, 11, 8, 6, 14, 15, 6, 13, 0, 8, 11, 8, 19, 0, 8, 11, 8, 6, 14, 15, 6, 13, 0, 8, 11, 8, 19, 0, 8, 11, 8, 6, 14, 15, 6, 13, 0, 8, 11, 8, 19, 0, 8, 11, 8, 6, 14, 15, 6, 13, 4, 4, 0, 8, 11, 8, 6, 1, 54, 48, 68, 63, 66, 58, 16, 7, 10, 13, 13, 14, 15, 13, 6, 13, 8, 11, 9, 7, 43, 7, 15, 13, 1, 54, 48, 68, 63, 51, 61, 48, 48, 16, 12, 15, 9, 9, 7, 15, 13, 11, 13]),\n",
       "       ...,\n",
       "       list([54, 48, 68, 58, 57, 48, 16, 13, 15, 14, 9, 7, 8, 12, 6, 11, 6, 1, 54, 48, 68, 63, 66, 58, 16, 13, 15, 7, 11, 11, 10, 8, 8, 14, 15]),\n",
       "       list([54, 48, 68, 58, 57, 48, 16, 7, 10, 13, 15, 6, 6, 15, 9, 9, 12, 14, 6, 8, 43, 7, 11, 12, 1, 54, 48, 68, 63, 66, 58, 16, 7, 10, 13, 15, 9, 13, 6, 7, 7, 11, 14, 15, 9, 43, 7, 15, 9, 1, 54, 48, 68, 63, 51, 61, 48, 48, 16, 44, 12, 6, 6, 10, 9, 6, 10, 44, 48, 47, 44, 47, 44, 6, 12, 49, 9, 48, 9, 7, 11, 9, 10, 14, 12, 6, 45, 47, 15, 44, 12]),\n",
       "       list([54, 48, 68, 58, 57, 48, 16, 7, 10, 13, 13, 6, 8, 7, 7, 7, 8, 6, 12, 13, 43, 8, 7, 12, 1, 54, 48, 68, 63, 66, 58, 16, 13, 7, 15, 14, 15, 14, 6, 11, 15, 13])], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = np.arange(len(x_batch))\n",
    "np.random.shuffle(index)\n",
    "\n",
    "x_batch = x_batch[index]\n",
    "y_batch = y_batch[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5000*0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 70)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69, 70)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1670"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Batcher():\n",
    "    def __init__(self, x, y):\n",
    "        self.train_size = int(len(x)*0.8)\n",
    "        self.train_x = x[:self.train_size]\n",
    "        self.train_y = y[:self.train_size]\n",
    "        self.test_x = x[self.train_size:]\n",
    "        self.test_y = y[self.train_size:]\n",
    "        self.start = 0\n",
    "    def next_batch(self, batch_size):\n",
    "        s_index = self.start\n",
    "        e_index = self.start + batch_size\n",
    "        if e_index >= self.train_size:\n",
    "            self.start = 0\n",
    "            s_index = self.start\n",
    "            e_index = self.start + batch_size\n",
    "        self.start = e_index\n",
    "        return self.train_x[s_index:e_index], self.train_y[s_index:e_index], \\\n",
    "                [len(w) for w in self.train_x[s_index:e_index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义一些常量\n",
    "#图片大小，32 x 256\n",
    "OUTPUT_SHAPE = (32,256)\n",
    "\n",
    "#训练最大轮次\n",
    "num_epochs = 10000\n",
    "#LSTM\n",
    "num_hidden = 64\n",
    "num_layers = 1\n",
    "num_classes = 2\n",
    "\n",
    "# obj = gen_id_card()\n",
    "# num_classes = obj.len + 1 + 1  # 10位数字 + blank + ctc blank\n",
    "\n",
    "#初始化学习速率\n",
    "INITIAL_LEARNING_RATE = 1e-3\n",
    "DECAY_STEPS = 5000\n",
    "REPORT_STEPS = 100\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.9  # The learning rate decay factor\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "DIGITS='0123456789'\n",
    "BATCHES = 20\n",
    "BATCH_SIZE = 500\n",
    "TRAIN_SIZE = BATCHES * BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_model():\n",
    "    inputs = tf.placeholder(tf.float32, [None, None, OUTPUT_SHAPE[0]])\n",
    "    targets = tf.sparse_placeholder(tf.int32)\n",
    "    seq_len = tf.placeholder(tf.int32, [None])\n",
    "    \n",
    "    with tf.name_scope(\"lstm\"):\n",
    "        # LSTM\n",
    "        cell = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=True)\n",
    "        stack = tf.contrib.rnn.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "        outputs, _ = tf.nn.dynamic_rnn(cell, inputs, seq_len, dtype=tf.float32)\n",
    "\n",
    "        shape = tf.shape(inputs)\n",
    "        # [batch_size,256]\n",
    "        batch_s, max_timesteps = shape[0], shape[1]\n",
    "\n",
    "        # [batch_size*max_time_step,num_hidden]\n",
    "        outputs = tf.reshape(outputs, [-1, num_hidden])\n",
    "        W = tf.Variable(tf.truncated_normal([num_hidden,  num_classes], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.0, shape=[num_classes]), name=\"b\")\n",
    "        # [batch_size*max_timesteps,num_classes]\n",
    "        logits = tf.matmul(outputs, W) + b\n",
    "        # [batch_size,max_timesteps,num_classes]\n",
    "        logits = tf.reshape(logits, [batch_s, -1, num_classes])\n",
    "        # 转置矩阵，第0和第1列互换位置=>[max_timesteps,batch_size,num_classes]\n",
    "        logits = tf.transpose(logits, (1, 0, 2))\n",
    "    \n",
    "    return logits, inputs, targets, seq_len, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n",
    "                                                global_step,\n",
    "                                                DECAY_STEPS,\n",
    "                                                LEARNING_RATE_DECAY_FACTOR,\n",
    "                                                staircase=True)\n",
    "with tf.name_scope(\"rnn\"):\n",
    "    logits, inputs, targets, seq_len, W, b = get_train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tragets是一个稀疏矩阵\n",
    "loss = tf.nn.ctc_loss(labels=targets,inputs=logits, sequence_length=seq_len)\n",
    "cost = tf.reduce_mean(loss)\n",
    "    \n",
    "#optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=MOMENTUM).minimize(cost, global_step=global_step)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss,global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_batch():\n",
    "#     train_inputs, train_targets, train_seq_len = get_next_batch(BATCH_SIZE)\n",
    "    train_inputs, train_targets, train_seq_len = sqli.next_batch(BATCH_SIZE)\n",
    "\n",
    "    feed = {inputs: train_inputs, targets: train_targets, seq_len: train_seq_len}\n",
    "\n",
    "    b_loss,b_targets, b_logits, b_seq_len,b_cost, steps, _ = session.run([loss, targets, logits, seq_len, cost, global_step, optimizer], feed)\n",
    "\n",
    "    print(b_cost, steps)\n",
    "    if steps > 0 and steps % REPORT_STEPS == 0:\n",
    "#         do_report()\n",
    "        save_path = saver.save(session, \"ocr.model\", global_step=steps)\n",
    "    return b_cost, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.transform(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_batch = [np.eye(len(vocab))[item] for item in x_batch]\n",
    "# x_batch = [np.eye(len(vocab))[item] for item in x_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sqli' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-4a99cfa830d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqli\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sqli' is not defined"
     ]
    }
   ],
   "source": [
    "len(sqli.next_batch(BATCH_SIZE)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sqli' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-89c787630ac0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msqli\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sqli' is not defined"
     ]
    }
   ],
   "source": [
    "sqli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch....... 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-7f434b0c0df3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCHES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mtrain_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mseconds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-0d55241ff483>\u001b[0m in \u001b[0;36mdo_batch\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtrain_seq_len\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mb_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_logits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_seq_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb_cost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_cost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    966\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \"\"\"\n\u001b[1;32m--> 531\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sqli = Batcher(x_batch,y_batch)\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=100)\n",
    "    for curr_epoch in range(num_epochs):\n",
    "        print(\"Epoch.......\", curr_epoch)\n",
    "        train_cost = train_ler = 0\n",
    "        for batch in range(BATCHES):\n",
    "            start = time.time()\n",
    "            c, steps = do_batch()\n",
    "            train_cost += c * BATCH_SIZE\n",
    "            seconds = time.time() - start\n",
    "            print(\"Step:\", steps, \", batch seconds:\", seconds)\n",
    "            \n",
    "        train_cost /= TRAIN_SIZE\n",
    "            \n",
    "#         train_inputs, train_targets, train_seq_len = get_next_batch(BATCH_SIZE)\n",
    "#         train_inputs, train_targets, train_seq_len = sqli.next_batch(BATCH_SIZE)\n",
    "        train_inputs = sqli.test_x\n",
    "        train_targets = sqli.test_y\n",
    "        train_seq_len = [len(w) for w in train_inputs]\n",
    "        val_feed = {inputs: train_inputs,\n",
    "                        targets: train_targets,\n",
    "                        seq_len: train_seq_len}\n",
    " \n",
    "        val_cost, val_ler, lr, steps = session.run([cost, acc, learning_rate, global_step], feed_dict=val_feed)\n",
    " \n",
    "        log = \"Epoch {}/{}, steps = {}, train_cost = {:.3f}, train_ler = {:.3f}, val_cost = {:.3f}, val_ler = {:.3f}, time = {:.3f}s, learning_rate = {}\"\n",
    "        print(log.format(curr_epoch + 1, num_epochs, steps, train_cost, train_ler, val_cost, val_ler, time.time() - start, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n",
    "                                                global_step,\n",
    "                                                DECAY_STEPS,\n",
    "                                                LEARNING_RATE_DECAY_FACTOR,\n",
    "                                                staircase=True)\n",
    "    logits, inputs, targets, seq_len, W, b = get_train_model()\n",
    "    \n",
    "    # tragets是一个稀疏矩阵\n",
    "    loss = tf.nn.ctc_loss(labels=targets,inputs=logits, sequence_length=seq_len)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    \n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=MOMENTUM).minimize(cost, global_step=global_step)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss,global_step=global_step)\n",
    "    \n",
    "    #前面说的划分块之后找每块的类属概率分布，ctc_beam_search_decoder方法,是每次找最大的K个概率分布\n",
    "    #还有一种贪心策略是只找概率最大那个，也就是K=1的情况ctc_ greedy_decoder\n",
    "    decoded, log_prob = tf.nn.ctc_beam_search_decoder(logits, seq_len, merge_repeated=False)\n",
    "    \n",
    "    acc = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32), targets))\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    def report_accuracy(decoded_list, test_targets):\n",
    "        original_list = decode_sparse_tensor(test_targets)\n",
    "        detected_list = decode_sparse_tensor(decoded_list)\n",
    "        true_numer = 0\n",
    "        \n",
    "        if len(original_list) != len(detected_list):\n",
    "            print(\"len(original_list)\", len(original_list), \"len(detected_list)\", len(detected_list),\n",
    "                  \" test and detect length desn't match\")\n",
    "            return\n",
    "        print(\"T/F: original(length) <-------> detectcted(length)\")\n",
    "        for idx, number in enumerate(original_list):\n",
    "            detect_number = detected_list[idx]\n",
    "            hit = (number == detect_number)\n",
    "            print(hit, number, \"(\", len(number), \") <-------> \", detect_number, \"(\", len(detect_number), \")\")\n",
    "            if hit:\n",
    "                true_numer = true_numer + 1\n",
    "        print(\"Test Accuracy:\", true_numer * 1.0 / len(original_list))\n",
    "\n",
    "    def do_report():\n",
    "        test_inputs,test_targets,test_seq_len = get_next_batch(BATCH_SIZE)\n",
    "        test_feed = {inputs: test_inputs,\n",
    "                     targets: test_targets,\n",
    "                     seq_len: test_seq_len}\n",
    "        dd, log_probs, accuracy = session.run([decoded[0], log_prob, acc], test_feed)\n",
    "        report_accuracy(dd, test_targets)\n",
    " \n",
    "    def do_batch():\n",
    "        train_inputs, train_targets, train_seq_len = get_next_batch(BATCH_SIZE)\n",
    "        \n",
    "        feed = {inputs: train_inputs, targets: train_targets, seq_len: train_seq_len}\n",
    "        \n",
    "        b_loss,b_targets, b_logits, b_seq_len,b_cost, steps, _ = session.run([loss, targets, logits, seq_len, cost, global_step, optimizer], feed)\n",
    "        \n",
    "        print(b_cost, steps)\n",
    "        if steps > 0 and steps % REPORT_STEPS == 0:\n",
    "            do_report()\n",
    "            save_path = saver.save(session, \"ocr.model\", global_step=steps)\n",
    "        return b_cost, steps\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=100)\n",
    "        for curr_epoch in xrange(num_epochs):\n",
    "            print(\"Epoch.......\", curr_epoch)\n",
    "            train_cost = train_ler = 0\n",
    "            for batch in xrange(BATCHES):\n",
    "                start = time.time()\n",
    "                c, steps = do_batch()\n",
    "                train_cost += c * BATCH_SIZE\n",
    "                seconds = time.time() - start\n",
    "                print(\"Step:\", steps, \", batch seconds:\", seconds)\n",
    "            \n",
    "            train_cost /= TRAIN_SIZE\n",
    "            \n",
    "            train_inputs, train_targets, train_seq_len = get_next_batch(BATCH_SIZE)\n",
    "            val_feed = {inputs: train_inputs,\n",
    "                        targets: train_targets,\n",
    "                        seq_len: train_seq_len}\n",
    " \n",
    "            val_cost, val_ler, lr, steps = session.run([cost, acc, learning_rate, global_step], feed_dict=val_feed)\n",
    " \n",
    "            log = \"Epoch {}/{}, steps = {}, train_cost = {:.3f}, train_ler = {:.3f}, val_cost = {:.3f}, val_ler = {:.3f}, time = {:.3f}s, learning_rate = {}\"\n",
    "            print(log.format(curr_epoch + 1, num_epochs, steps, train_cost, train_ler, val_cost, val_ler, time.time() - start, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.read_csv(\"./data/test.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.read_csv(\"./data/test2.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A Bidirectional Recurrent Neural Network (LSTM) implementation example using TensorFlow library.\n",
    "This example is using the MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "Long Short Term Memory paper: http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n",
    "\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_y = mnist.train.next_batch(100000)\n",
    "batch_x.shape, batch_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "To classify images using a bidirectional recurrent neural network, we consider\n",
    "every image row as a sequence of pixels. Because MNIST image shape is 28*28px,\n",
    "we will then handle 28 sequences of 28 steps for every sample.\n",
    "'''\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 可以理解为，训练时总共用的样本数\n",
    "training_iters = 100000\n",
    "\n",
    "# 每次训练的样本大小\n",
    "batch_size = 128\n",
    "\n",
    "# 这个是用来显示的。\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "# n_steps*n_input其实就是那张图 把每一行拆到每个time step上。\n",
    "n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28 # timesteps\n",
    "\n",
    "# 隐藏层大小\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "# [None, n_steps, n_input]这个None表示这一维不确定大小\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    # Hidden layer weights => 2*n_hidden because of forward + backward cells\n",
    "    'out': tf.Variable(tf.random_normal([2*n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def BiRNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `bidirectional_rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    # 变成了n_steps*(batch_size, n_input)\n",
    "    x = tf.unstack(x, n_steps, 1)\n",
    "\n",
    "    # Define lstm cells with tensorflow\n",
    "    # Forward direction cell\n",
    "    lstm_fw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "    # Backward direction cell\n",
    "    lstm_bw_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    try:\n",
    "        outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                              dtype=tf.float32)\n",
    "    except Exception: # Old TensorFlow version only returns outputs not states\n",
    "        outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n",
    "                                        dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "pred = BiRNN(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "# softmax_cross_entropy_with_logits：Measures the probability error in discrete classification tasks in which the classes are mutually exclusive\n",
    "# return a 1-D Tensor of length batch_size of the same type as logits with the softmax cross entropy loss.\n",
    "# reduce_mean就是对所有数值（这里没有指定哪一维）求均值。\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [5,6,7]\n",
    "depth = 1\n",
    "one_hot = tf.one_hot(indices, depth) \n",
    "with tf.Session() as sess:\n",
    "    res = sess.run(one_hot)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.identity(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.identity(max(b)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [5, 4, 5] \n",
    "b = [1, 2, 3]\n",
    "# one hot an integer\n",
    "one_hot_a = tf.nn.embedding_lookup(np.identity(10), a)\n",
    "# one hot a list of integers\n",
    "one_hot_b = tf.nn.embedding_lookup(np.identity(max(b)+1), b)\n",
    "with tf.Session() as sess:\n",
    "    res, res2 = sess.run([one_hot_a, one_hot_b])\n",
    "    print(res)\n",
    "    print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
