{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get word_list & vocab_list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 30000/30000 [00:00<00:00, 105725.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get vocab\n",
      "['%' '&' '*' '+' '-' '.' '0' '1' '2' '3' '4' '5' '6' '7' '8' '9' '=' 'A'\n",
      " 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S'\n",
      " 'T' 'U' 'V' 'W' 'X' 'Y' 'Z' '_' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j'\n",
      " 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n",
      "lable encoder vocab\n",
      "['%' '&' '*' '+' '-' '.' '0' '1' '2' '3' '4' '5' '6' '7' '8' '9' '=' 'A'\n",
      " 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S'\n",
      " 'T' 'U' 'V' 'W' 'X' 'Y' 'Z' '_' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j'\n",
      " 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n",
      "label encoder word_list\n",
      "balanced pos & neg data\n",
      "Counter({0: 27159, 1: 1973})\n",
      "[ 1183 12483 10590 ..., 14963  8739 27675] (1973,)\n",
      "[ 5035 21715 19312 ..., 28305 17363 15599] (2027,)\n",
      "[25611 23848 25834 ..., 23176  2268 24616] (4000,)\n",
      "get data\n",
      "(4000,) (4000,) 4000\n",
      "padding & one-hot x data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 4000/4000 [00:00<00:00, 37283.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding & one-hot y data\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "train_pd = pd.read_csv(\"./data/train.csv\", encoding=\"utf-8\")\n",
    "\n",
    "print(\"get word_list & vocab_list\")\n",
    "word_list = []\n",
    "vocab_list = []\n",
    "for line in tqdm(train_pd.value):\n",
    "    word_list.append([w for w in line])\n",
    "    vocab_list.extend([w for w in line])\n",
    "    \n",
    "print(\"get vocab\")\n",
    "c = Counter(vocab_list)\n",
    "vocab = np.array(list(c.keys()))\n",
    "vocab.sort()\n",
    "print(vocab)\n",
    "\n",
    "print(\"lable encoder vocab\")\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(vocab)\n",
    "print(label_encoder.classes_)\n",
    "# label_encoder.transform(vocab)\n",
    "\n",
    "print(\"label encoder word_list\")\n",
    "word_labelencoder = [list(label_encoder.transform(w)) for w in word_list]\n",
    "word_size = [len(i) for i in word_labelencoder]\n",
    "\n",
    "print(\"balanced pos & neg data\")\n",
    "train_size_pd = pd.DataFrame(word_size, columns=['len'])\n",
    "train_size_pd['label'] = train_pd.label\n",
    "# train_size_pd.describe()\n",
    "train_size_pd = train_size_pd[train_size_pd.len < 500]\n",
    "print(Counter(train_size_pd.label))\n",
    "\n",
    "pos_index = train_size_pd[train_size_pd.label == 1].index.values\n",
    "random.shuffle(pos_index)\n",
    "print(pos_index, pos_index.shape)\n",
    "\n",
    "neg_index = train_size_pd[train_size_pd.label == 0].index.values\n",
    "neg_index = np.random.choice(neg_index, size=4000-1973)\n",
    "print(neg_index, neg_index.shape)\n",
    "\n",
    "balance_train_index = np.append(pos_index,neg_index)\n",
    "random.shuffle(balance_train_index)\n",
    "print(balance_train_index, balance_train_index.shape)\n",
    "\n",
    "print(\"get data\")\n",
    "x_batch = np.array(word_labelencoder.copy())\n",
    "x_batch = x_batch[balance_train_index]\n",
    "y_batch = np.array(train_pd.label.copy())\n",
    "y_batch = y_batch[balance_train_index]\n",
    "x_batch_size = [len(i) for i in x_batch]\n",
    "print(x_batch.shape, y_batch.shape, len(x_batch_size))\n",
    "\n",
    "print(\"padding & one-hot x data\")\n",
    "max_size = np.max(x_batch_size)\n",
    "x_batch_pad = []\n",
    "for x in tqdm(x_batch[:]):\n",
    "    list_test = list()\n",
    "    list_test =([-1] * max_size)\n",
    "    list_test[:len(x)] = x\n",
    "#     list_test = [[i] for i in list_test]\n",
    "    x_batch_pad.append(list_test)\n",
    "x_batch_pad = [np.eye(len(vocab))[item] for item in x_batch_pad]\n",
    "   \n",
    "print(\"padding & one-hot y data\")\n",
    "y_batch_pad = [np.eye(2)[item] for item in y_batch]\n",
    "y_batch_pad = [list(i) for i in y_batch_pad]\n",
    "\n",
    "print(\"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Batcher():\n",
    "    def __init__(self, x, y, x_batch_size):\n",
    "        self.split_size = int(len(x)*0.8)\n",
    "        self.train_x = x[:self.split_size]\n",
    "        self.train_y = y[:self.split_size]\n",
    "        self.train_size = x_batch_size[:self.split_size]\n",
    "        self.test_x = x[self.split_size:]\n",
    "        self.test_y = y[self.split_size:]\n",
    "        self.test_size = x_batch_size[self.split_size:]\n",
    "        self.start = 0\n",
    "    def next_batch(self, batch_size):\n",
    "        s_index = self.start\n",
    "        e_index = self.start + batch_size\n",
    "        if e_index >= self.split_size:\n",
    "            self.start = 0\n",
    "            s_index = self.start\n",
    "            e_index = self.start + batch_size\n",
    "        self.start = e_index\n",
    "        return self.train_x[s_index:e_index], self.train_y[s_index:e_index], self.train_size[s_index:e_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqli_batch = Batcher(x_batch_pad, y_batch_pad, x_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_data, batch_labels, batch_seqlen = sqli_batch.next_batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  1.]]),\n",
       " array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  1.]])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 1.0],\n",
       " [1.0, 0.0],\n",
       " [0.0, 1.0],\n",
       " [1.0, 0.0],\n",
       " [0.0, 1.0],\n",
       " [1.0, 0.0],\n",
       " [1.0, 0.0],\n",
       " [0.0, 1.0],\n",
       " [1.0, 0.0],\n",
       " [0.0, 1.0]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[156, 41, 350, 56, 383, 119, 101, 437, 246, 143]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'bi-GRU/bidirectional_rnn/fw/fw/transpose:0' shape=(?, 499, 128) dtype=float32>, <tf.Tensor 'bi-GRU/ReverseSequence:0' shape=(?, 499, 128) dtype=float32>)\n",
      "Tensor(\"bi-GRU/concat:0\", shape=(?, 499, 256), dtype=float32)\n",
      "Tensor(\"bi-GRU/Reshape:0\", shape=(?, 256), dtype=float32)\n",
      "xw_plus_b: Tensor(\"output/xw_plus_b:0\", shape=(?, 2), dtype=float32)\n",
      "logits: Tensor(\"output/Reshape:0\", shape=(?, 499, 2), dtype=float32)\n",
      "Tensor(\"loss/GatherV2:0\", shape=(?, 499, 1), dtype=float32)\n",
      "logits_label: Tensor(\"loss/Reshape:0\", shape=(?, 499), dtype=float32)\n",
      "Tensor(\"loss/Cast:0\", shape=(?,), dtype=int32)\n",
      "499 100\n",
      "Tensor(\"loss/add:0\", shape=(?,), dtype=int32)\n",
      "Tensor(\"output/Reshape:0\", shape=(?, 499, 2), dtype=float32)\n",
      "Tensor(\"loss/Gather:0\", shape=(?, 2), dtype=float32)\n",
      "logits_max: Tensor(\"loss/Reshape_2:0\", shape=(?, 2), dtype=float32) Tensor(\"Placeholder_1:0\", shape=(?, 2), dtype=float32)\n",
      "loss: Tensor(\"loss/loss_1:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict: Tensor(\"valid/ArgMax:0\", shape=(?,), dtype=int64)\n",
      "y_arg: Tensor(\"valid/ArgMax_1:0\", shape=(?,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_steps = 500\n",
    "batch_size = 100\n",
    "display_step = 50\n",
    "\n",
    "# Network Parameters\n",
    "seq_max_len = max_size # Sequence max length\n",
    "forward_units = 128 # hidden layer num of features\n",
    "backward_units = 128 # hidden layer num of features\n",
    "n_classes = 2 # linear sequence or not\n",
    "\n",
    "# Batcher zeroday_batch\n",
    "sqli_batch = Batcher(x_batch_pad, y_batch_pad, x_batch_size)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, seq_max_len, len(vocab)])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# A placeholder for indicating each sequence length\n",
    "seqlen = tf.placeholder(tf.int64, [None,])\n",
    "\n",
    "    \n",
    "with tf.variable_scope(\"bi-GRU\") as scope:\n",
    "    # Define a lstm cell with tensorflow\n",
    "    encoder_fw = tf.contrib.rnn.GRUCell(forward_units)\n",
    "    encoder_bw = tf.contrib.rnn.GRUCell(backward_units)\n",
    "\n",
    "    outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_fw, \n",
    "                                            cell_bw=encoder_bw, \n",
    "                                            inputs=x,\n",
    "                                            sequence_length=seqlen,\n",
    "                                            dtype=tf.float32)\n",
    "\n",
    "    print(outputs)\n",
    "    output = tf.concat(outputs, 2)\n",
    "    print(output)\n",
    "#     batch_size = output.get_shape().as_list()[0]\n",
    "    reshape = tf.reshape(output, [-1, forward_units + backward_units])\n",
    "    print(reshape)\n",
    "    \n",
    "with tf.variable_scope(\"output\"):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\",\n",
    "                                shape=[forward_units + backward_units, n_classes],\n",
    "                                initializer=tf.truncated_normal_initializer(stddev=0.05),\n",
    "                                dtype=tf.float32)\n",
    "    softmax_b = tf.get_variable(\"softmax_b\",\n",
    "                                shape=[n_classes],\n",
    "                                initializer=tf.constant_initializer(value=0.),\n",
    "                                dtype=tf.float32)\n",
    "    xw_plus_b = tf.nn.xw_plus_b(reshape, softmax_w, softmax_b)\n",
    "    print(\"xw_plus_b:\", xw_plus_b)\n",
    "    logits = tf.reshape(xw_plus_b, [-1, seq_max_len, n_classes])\n",
    "    print(\"logits:\", logits)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    print(tf.gather(logits, [1], axis=2))\n",
    "    logits_label = tf.reshape(tf.gather(logits, [1], axis=2), [-1, seq_max_len])\n",
    "    print(\"logits_label:\", logits_label)\n",
    "#     logits_score = tf.reduce_max(logits_label, axis=1)\n",
    "    logits_index = tf.cast(tf.argmax(logits_label, axis=1), tf.int32)\n",
    "    print(logits_index)\n",
    "    print(seq_max_len, batch_size)\n",
    "    batch_size_ = tf.shape(logits)[0]\n",
    "    logits_index = tf.range(0, batch_size_) * seq_max_len + logits_index\n",
    "    print(logits_index)\n",
    "    print(logits)\n",
    "    logits_reshape = tf.reshape(logits, [-1, n_classes])\n",
    "    print(tf.gather(logits_reshape, logits_index, axis=0))\n",
    "    logits_max = tf.reshape(tf.gather(logits_reshape, logits_index, axis=0), [-1, n_classes])\n",
    "    print(\"logits_max:\", logits_max, y)\n",
    "    loss_per_example_average = tf.nn.softmax_cross_entropy_with_logits(logits=logits_max, labels=y, name=\"loss\")\n",
    "#     print(fake_loss)\n",
    "#     y_sum = tf.reduce_sum(y, axis=1)\n",
    "#     print(y_sum)\n",
    "#     mask = tf.cast(tf.sign(y_sum), dtype=tf.float32)\n",
    "#     print(mask)\n",
    "#     loss_per_example_per_step = tf.multiply(fake_loss, mask)\n",
    "#     print(loss_per_example_per_step)\n",
    "#     loss_per_example_sum = tf.reduce_sum(loss_per_example_per_step, reduction_indices=[1])\n",
    "#     print(loss_per_example_sum)\n",
    "#     loss_per_example_average = tf.div(x=loss_per_example_sum,\n",
    "#                                       y=tf.cast(seqlen, tf.float32))\n",
    "#     print(loss_per_example_average)\n",
    "    loss = tf.reduce_mean(loss_per_example_average, name=\"loss\")\n",
    "    print(\"loss:\", loss)\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss, name=\"train_op\")\n",
    "    \n",
    "with tf.name_scope(\"valid\"):\n",
    "    predict = tf.argmax(logits_max, axis=1)\n",
    "    print(\"predict:\", predict)\n",
    "    y_arg = tf.argmax(y, axis=1)\n",
    "    print(\"y_arg:\", y_arg)\n",
    "    accuracy_per_example = tf.cast(tf.equal(predict, y_arg), dtype=tf.float32, name=\"valid_accuracy\")\n",
    "#     accuracy_matrix = tf.multiply(fake_accuracy, mask)\n",
    "#     accuracy_per_example = tf.div(x=tf.reduce_sum(accuracy_matrix, 1),\n",
    "#                                   y=tf.cast(seqlen, tf.float32))\n",
    "    accuracy = tf.reduce_mean(accuracy_per_example, name=\"valid_accuracy\")\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, Minibatch Loss= 0.578663, Training Accuracy= 0.53000\n",
      "Testing Accuracy: 0.49\n",
      "Step 5000, Minibatch Loss= 0.430069, Training Accuracy= 0.95000\n",
      "Testing Accuracy: 0.93375\n",
      "Step 10000, Minibatch Loss= 0.411055, Training Accuracy= 0.89000\n",
      "Testing Accuracy: 0.88375\n",
      "Step 15000, Minibatch Loss= 0.159367, Training Accuracy= 0.99000\n",
      "Testing Accuracy: 0.98625\n",
      "Step 20000, Minibatch Loss= 0.081968, Training Accuracy= 1.00000\n",
      "Testing Accuracy: 0.98375\n",
      "Step 25000, Minibatch Loss= 0.067482, Training Accuracy= 0.99000\n",
      "Testing Accuracy: 0.9725\n",
      "Step 30000, Minibatch Loss= 0.094017, Training Accuracy= 0.98000\n",
      "Testing Accuracy: 0.9825\n",
      "Step 35000, Minibatch Loss= 0.024694, Training Accuracy= 1.00000\n",
      "Testing Accuracy: 0.98875\n",
      "Step 40000, Minibatch Loss= 0.052463, Training Accuracy= 0.99000\n",
      "Testing Accuracy: 0.98625\n",
      "Step 45000, Minibatch Loss= 0.027828, Training Accuracy= 1.00000\n",
      "Testing Accuracy: 0.98625\n",
      "Step 50000, Minibatch Loss= 0.032296, Training Accuracy= 1.00000\n",
      "Testing Accuracy: 0.9925\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9925\n"
     ]
    }
   ],
   "source": [
    "LOG_DIR = \"./logs/biGRU-07/\"\n",
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=15)\n",
    "module_file = tf.train.latest_checkpoint(LOG_DIR)\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    tf.summary.scalar(\"acc\", accuracy)\n",
    "        \n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(LOG_DIR + \"train/\", sess.graph)\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "#     saver.restore(sess, module_file)\n",
    "\n",
    "    for step in range(1, training_steps + 1):\n",
    "        # Get batch data\n",
    "        batch_x, batch_y, batch_seqlen = sqli_batch.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        _, summary_ = sess.run([train_op, merged], feed_dict={x: batch_x, y: batch_y, seqlen: batch_seqlen})\n",
    "        train_writer.add_summary(summary_, step)\n",
    "        \n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch accuracy & loss\n",
    "            acc, cost = sess.run([accuracy, loss], feed_dict={x: batch_x, y: batch_y,\n",
    "                                                seqlen: batch_seqlen})\n",
    "            print(\"Step \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(cost) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            test_data = sqli_batch.test_x\n",
    "            test_label = sqli_batch.test_y\n",
    "            test_seqlen = sqli_batch.test_size\n",
    "            test_acc = sess.run(accuracy, feed_dict={x: test_data, y: test_label, seqlen: test_seqlen})\n",
    "            print(\"Testing Accuracy:\", test_acc)\n",
    "        if step % 5 == 0:\n",
    "            saver.save(sess, LOG_DIR+\"sqli.ckpt\", global_step=step)\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    test_data = sqli_batch.test_x\n",
    "    test_label = sqli_batch.test_y\n",
    "    test_seqlen = sqli_batch.test_size\n",
    "    test_acc = sess.run(accuracy, feed_dict={x: test_data, y: test_label, seqlen: test_seqlen})\n",
    "    print(\"Testing Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./logs/biGRU-07/sqli.ckpt-500\n",
      "Testing Accuracy: 0.9925\n"
     ]
    }
   ],
   "source": [
    "LOG_DIR = \"./logs/biGRU-07/\"\n",
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=15)\n",
    "module_file = tf.train.latest_checkpoint(LOG_DIR)\n",
    "\n",
    "test_pred = ''\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver.restore(sess, module_file)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    test_data = sqli_batch.test_x\n",
    "    test_label = sqli_batch.test_y\n",
    "    test_seqlen = sqli_batch.test_size\n",
    "    test_pred, test_acc = sess.run([logits_max, accuracy], feed_dict={x: test_data, y: test_label, seqlen: test_seqlen})\n",
    "    print(\"Testing Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff len: 6\n",
      "get test data\n",
      "get diff raw data\n",
      "test_pred_label [0 0 0 0 0 0]\n",
      "test_real_label [1 1 1 1 1 1]\n",
      "    test_data_y [1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "test_pred_label = np.argmax(test_pred, 1)\n",
    "test_real_label = np.argmax(test_label, 1)\n",
    "test_diff = test_pred_label != test_real_label\n",
    "test_diff_index = np.where(test_diff == True)[0]\n",
    "print(\"diff len:\", len(test_diff_index))\n",
    "\n",
    "x_batch_lbl = np.array(word_labelencoder.copy())\n",
    "x_batch_lbl = x_batch_lbl[balance_train_index]\n",
    "word_list_array = np.array(word_list)\n",
    "test_data = word_list_array[balance_train_index][int(0.8*len(balance_train_index)):]\n",
    "print(\"get test data\")\n",
    "\n",
    "diff_data = test_data[test_diff_index]\n",
    "diff_data_str = []\n",
    "for data in diff_data:\n",
    "    diff_data_str.append(\"\".join(data))\n",
    "#     print(\"\".join(data))\n",
    "print(\"get diff raw data\")\n",
    "\n",
    "test_data_all = np.array(train_pd.label.copy())\n",
    "test_data_y = test_data_all[balance_train_index][int(0.8*len(balance_train_index)):]\n",
    "\n",
    "print(\"test_pred_label\", test_pred_label[test_diff_index])\n",
    "print(\"test_real_label\", test_real_label[test_diff_index])\n",
    "print(\"    test_data_y\", test_data_y[test_diff_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diff_pd = pd.DataFrame(diff_data_str, columns=['urldata'])\n",
    "diff_pd['pred'] = test_pred_label[test_diff_index]\n",
    "diff_pd['real'] = test_real_label[test_diff_index]\n",
    "diff_pd['label'] = test_data_y[test_diff_index]\n",
    "diff_pd.to_csv(\"./data/007#bi-GRU-MaxPool.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urldata</th>\n",
       "      <th>pred</th>\n",
       "      <th>real</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>keyone=7973846509&amp;keytwo=-1+union+select+group...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>keyone=-2+union+select+1%2C2%2Ctable_name%2C4%...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>keyone=1477732762810_64&amp;keytwo=1+-+%28binary+%...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>keyone=7975968234&amp;keytwo=ELT%25286477%253D6477...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>keyone=999999999+union+select+password+from+te...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>keyone=1477033826919_309&amp;keytwo=2&amp;keythree=aa%...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             urldata  pred  real  label\n",
       "0  keyone=7973846509&keytwo=-1+union+select+group...     0     1      1\n",
       "1  keyone=-2+union+select+1%2C2%2Ctable_name%2C4%...     0     1      1\n",
       "2  keyone=1477732762810_64&keytwo=1+-+%28binary+%...     0     1      1\n",
       "3  keyone=7975968234&keytwo=ELT%25286477%253D6477...     0     1      1\n",
       "4  keyone=999999999+union+select+password+from+te...     0     1      1\n",
       "5  keyone=1477033826919_309&keytwo=2&keythree=aa%...     0     1      1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 200000 = 2000 * 100\n",
    "# n_hidden = 128 \n",
    "Step 100, Minibatch Loss= 0.725667, Training Accuracy= 0.49000\n",
    "Step 10000, Minibatch Loss= 0.473397, Training Accuracy= 0.78000\n",
    "Step 20000, Minibatch Loss= 0.276774, Training Accuracy= 0.94000\n",
    "Step 30000, Minibatch Loss= 0.073246, Training Accuracy= 0.98000\n",
    "Step 40000, Minibatch Loss= 0.115629, Training Accuracy= 0.97000\n",
    "Step 50000, Minibatch Loss= 0.058722, Training Accuracy= 0.99000\n",
    "Step 60000, Minibatch Loss= 0.095137, Training Accuracy= 0.98000\n",
    "Step 70000, Minibatch Loss= 0.107541, Training Accuracy= 0.97000\n",
    "Step 80000, Minibatch Loss= 0.039399, Training Accuracy= 0.99000\n",
    "Step 90000, Minibatch Loss= 0.131407, Training Accuracy= 0.96000\n",
    "Step 100000, Minibatch Loss= 0.081308, Training Accuracy= 0.98000\n",
    "Step 110000, Minibatch Loss= 0.087308, Training Accuracy= 0.98000\n",
    "Step 120000, Minibatch Loss= 0.090227, Training Accuracy= 0.98000\n",
    "Step 130000, Minibatch Loss= 0.125701, Training Accuracy= 0.97000\n",
    "Step 140000, Minibatch Loss= 0.165360, Training Accuracy= 0.94000\n",
    "Step 150000, Minibatch Loss= 0.139225, Training Accuracy= 0.96000\n",
    "Step 160000, Minibatch Loss= 0.132529, Training Accuracy= 0.96000\n",
    "Step 170000, Minibatch Loss= 0.178868, Training Accuracy= 0.95000\n",
    "Step 180000, Minibatch Loss= 0.092851, Training Accuracy= 0.98000\n",
    "Step 190000, Minibatch Loss= 0.086010, Training Accuracy= 0.98000\n",
    "Step 200000, Minibatch Loss= 0.173377, Training Accuracy= 0.96000\n",
    "Optimization Finished!\n",
    "Testing Accuracy: 0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
